{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code Synthesis"
      ],
      "metadata": {
        "id": "NaiEJ7s62dER"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zppwvue-cBuL"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\" # this is needed to get rid of weird colab locale error\n",
        "# if you are still running into issues, please restart the runtime to initialize a new environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# installing the accelerate library\n",
        "!pip install accelerate"
      ],
      "metadata": {
        "id": "xeZRUs4nwlV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "def load_base_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-2B-mono\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-2B-mono\", device_map='auto', torch_dtype=torch.float16)\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_base_model()"
      ],
      "metadata": {
        "id": "WrdVC1gA1tTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/evalplus/evalplus\n",
        "!pip install evalplus==0.2.0"
      ],
      "metadata": {
        "id": "y5QGDzObv6OR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain the humaneval dataset\n",
        "from evalplus.data import get_human_eval_plus\n",
        "\n",
        "dataset = get_human_eval_plus()\n",
        "# feel free to play around the dataset to see what it looks like!"
      ],
      "metadata": {
        "id": "0CXZ2J-1ygY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make the folder to save the results\n",
        "!mkdir codegen_results"
      ],
      "metadata": {
        "id": "qV2Er_0IzwVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def program_synthesis(input_prompt: str, model) -> str:\n",
        "    # TODO: implement greedy sampling solution using codegen.\n",
        "    # note, you want to return a complete function here\n",
        "    # note, there might be some post processing needed to remove irrelevant\n",
        "    # tokens.\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def complete_base_humaneval(model, dataset, workdir):\n",
        "  for task_id, problem in tqdm(dataset.items()):\n",
        "      name = task_id.replace(\"/\", \"_\")\n",
        "\n",
        "      prompt = problem['prompt']\n",
        "\n",
        "      solution = program_synthesis(prompt, model)\n",
        "      os.makedirs(os.path.join(workdir, name), exist_ok=True)\n",
        "\n",
        "      with open(os.path.join(workdir, name, '0.py'), 'w') as f:\n",
        "          f.write(solution)\n",
        "\n",
        "\n",
        "# generate the solutions produced by codegen\n",
        "complete_base_humaneval(model, dataset, \"codegen_results\")"
      ],
      "metadata": {
        "id": "RUHkSvn20caK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now place take a look at the solutions produced by codegen in the folder\n",
        "# we will now evaluate the solution\n",
        "# note you can passing in \"--i-just-wanna-run\" to this command to\n",
        "# recompute the results IF and ONLY IF you have made some updates to each solution file :)\n",
        "!evalplus.evaluate --dataset humaneval --samples codegen_results"
      ],
      "metadata": {
        "id": "Wl9I1P1W3JNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def check_which_failed(workdir: str, dataset):\n",
        "    with open(os.path.join(workdir, \"eval_results.json\"), \"r\") as f:\n",
        "        results = json.loads(f.read())\n",
        "\n",
        "    failed_humaneval = []\n",
        "    failed_humaneval_plus = []\n",
        "\n",
        "    for task_id in dataset.keys():\n",
        "        total = results['eval'][task_id]['nfiles']\n",
        "        humaneval_base = len([x for x in results['eval'][task_id]['base'] if x[0] == \"success\"]) / total\n",
        "        humaneval_plus = len([x for x in results['eval'][task_id]['plus'] if x[0] == \"success\"]) / total\n",
        "\n",
        "        if humaneval_base != 1:\n",
        "            failed_humaneval.append(task_id)\n",
        "        if humaneval_plus != 1:\n",
        "            failed_humaneval_plus.append(task_id)\n",
        "\n",
        "    return failed_humaneval, failed_humaneval_plus\n"
      ],
      "metadata": {
        "id": "lw70jZp98NGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can use this to check which problem the model did not correctly solve\n",
        "failed_humaneval, failed_humaneval_plus = check_which_failed(\"codegen_results\", dataset)"
      ],
      "metadata": {
        "id": "ZH5o1fgV9Ng4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improve LLM Code Synthesis"
      ],
      "metadata": {
        "id": "Q6P8I67v9gOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make the folder to save the results\n",
        "!mkdir codegen_results_improved"
      ],
      "metadata": {
        "id": "dOVHXe_ue59n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def program_synthesis_improved(input_prompt: str, model, **kwargs) -> str:\n",
        "    # TODO: implement solution using codegen.\n",
        "    # similar to the previous function, you want to return the complete solution\n",
        "    # you may use additional parameters (use kwargs) to adjust\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def complete_improve_humaneval(model, dataset, workdir):\n",
        "  for task_id, problem in tqdm(dataset.items()):\n",
        "      name = task_id.replace(\"/\", \"_\")\n",
        "\n",
        "      prompt = problem['prompt']\n",
        "\n",
        "      solution = program_synthesis_improved(prompt, model)\n",
        "      os.makedirs(os.path.join(workdir, name), exist_ok=True)\n",
        "\n",
        "      with open(os.path.join(workdir, name, '0.py'), 'w') as f:\n",
        "          f.write(solution)\n",
        "\n",
        "\n",
        "# generate the solutions produced by codegen\n",
        "complete_improve_humaneval(model, dataset, \"codegen_results_improved\")"
      ],
      "metadata": {
        "id": "rAN0kxUjtSNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# note you can passing in \"--i-just-wanna-run\" to this command to\n",
        "# recompute the results IF and ONLY IF you have made some updates to each solution file :)\n",
        "# you may need to pass in (yes Y | command) on colab\n",
        "!yes Y | evalplus.evaluate --dataset humaneval --samples codegen_results_improved --i-just-wanna-run"
      ],
      "metadata": {
        "id": "MfM9aLs741F_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}